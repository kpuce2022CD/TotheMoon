import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import urllib.request
from bs4 import BeautifulSoup
import os
from konlpy.tag import Okt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import pickle
# tt
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
from tqdm import tqdm, tqdm_notebook
# kobert
from kobert.utils import get_tokenizer
from kobert.pytorch_kobert import get_pytorch_kobert_model
# transformers
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup
import tensorflow as tf

# debug(ê°œë°œì‹œ)>info(ìš´ì˜ì‹œ)>warnig(ì‚¬ìš©ìì˜ëª»ì…ë ¥)>error(ì˜ˆì™¸ë°œìƒ)>critical(ë°ì´í„°ì†ì‹¤,ì˜¤ì‘ë™)
logger = logging.getLogger("main")
stream_handler = logging.StreamHandler()
logger.addHandler(stream_handler)
logger.setLevel(logging.DEBUG)

# ------------------------------------- ê¸ì • ë¶€ì • ë¶„ë¥˜ (start)--------------------------------------------
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

okt = Okt()
f = open("stopwords.txt", 'r', encoding='utf8')
stopwords = f.read()
# ì •ìˆ˜ ì¸ì½”ë”©

max_len = 30

# loading
with open('np_tokenizer.pickle', 'rb') as handle:
    NP_tokenizer = pickle.load(handle)

loaded_model = load_model('np_classify_model.h5')


def sentiment_predict(new_sentence):
    new_sentence = re.sub(r'[^ã„±-ã…ã…-ã…£ê°€-í£ ]', '', new_sentence)
    new_sentence = okt.morphs(new_sentence, stem=True)  # í† í°í™”
    new_sentence = [word for word in new_sentence if not word in stopwords]  # ë¶ˆìš©ì–´ ì œê±°
    encoded = NP_tokenizer.texts_to_sequences([new_sentence])  # ì •ìˆ˜ ì¸ì½”ë”©
    pad_new = pad_sequences(encoded, maxlen=max_len)  # íŒ¨ë”©
    score = float(loaded_model.predict(pad_new))  # ì˜ˆì¸¡
    return score


# ----------------------------------------------------------------------------------------------------

# ----------------------------------------- kobert (ê°ì • ë¶„ë¥˜ start)---------------------------------------

# GPU ì‚¬ìš©
USE_CUDA = torch.cuda.is_available()  # gpu ì„¸íŒ… í™•ì¸ -> true or false
print(USE_CUDA)
# device = torch.device('cuda:0')  # gpu ì‚¬ìš© ì½”ë“œ
device = torch.device('cpu')  # cpu ì‚¬ìš© ì½”ë“œ

# BERT ëª¨ë¸, Vocabulary ë¶ˆëŸ¬ì˜¤ê¸°
bertmodel, vocab = get_pytorch_kobert_model()

# í† í°í™”
EM_tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(EM_tokenizer, vocab, lower=False)

# Setting parameters
max_len = 64
batch_size = 128
warmup_ratio = 0.1
num_epochs = 5
max_grad_norm = 1
log_interval = 200
learning_rate = 5e-5
num_workers = 0


class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i],))

    def __len__(self):
        return (len(self.labels))


class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size=768,
                 num_classes=7,  ##í´ë˜ìŠ¤ ìˆ˜ ì¡°ì •##
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate

        self.classifier = nn.Linear(hidden_size, num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)

    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)

        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(),
                              attention_mask=attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)


model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)
model.load_state_dict(torch.load('model_state_dic.pt', map_location=device))


# model = torch.load('em_classify_model.pt',map_location=device)  # íŒŒì´í† ì¹˜ ëª¨ë¸ ë¡œë“œ

def emotion_predict(predict_sentence):
    data = [predict_sentence, '0']
    dataset_another = [data]

    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)
    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=0)

    model.eval()

    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)

        valid_length = valid_length
        label = label.long().to(device)

        out = model(token_ids, valid_length, segment_ids)

        test_eval = ["ê³µí¬ê°€", "ë†€ëŒì´", "ë¶„ë…¸ê°€", "ìŠ¬í””ì´", "ì¤‘ë¦½ì´", "í–‰ë³µì´", "í˜ì˜¤ê°€"]
        for i in out:
            logits = i
            logits = logits.detach().cpu().numpy()
            probabilities = tf.nn.softmax(logits, axis=-1)
            probabilitiyValue = probabilities.numpy()
            probabilitiyIndexValue = probabilitiyValue[np.argmax(logits)] * 100
            probabilitiyFormatValue = round(probabilitiyIndexValue, 0)

            if np.argmax(logits) == 0:
                if (probabilitiyFormatValue > 80):
                    print(
                        ">> ì…ë ¥í•˜ì‹  ë‚´ìš©ì—ì„œ " + f"{probabilitiyFormatValue}ì˜ í™•ë¥ ë¡œ " + test_eval[np.argmax(logits)] + " ëŠê»´ì§‘ë‹ˆë‹¤.")
                    return np.argmax(logits)
            elif np.argmax(logits) == 1:
                if (probabilitiyFormatValue > 80):
                    print(
                        "ëŒ“ê¸€ ë‚´ìš©ì—ì„œ " + f"{probabilitiyFormatValue}ì˜ í™•ë¥ ë¡œ " + test_eval[np.argmax(logits)] + " ëŠê»´ì§‘ë‹ˆë‹¤.")
                    return np.argmax(logits)
            elif np.argmax(logits) == 2:
                if (probabilitiyFormatValue > 80):
                    print(
                        "ëŒ“ê¸€ ë‚´ìš©ì—ì„œ " + f"{probabilitiyFormatValue}ì˜ í™•ë¥ ë¡œ " + test_eval[np.argmax(logits)] + " ëŠê»´ì§‘ë‹ˆë‹¤.")
                    return np.argmax(logits)
            elif np.argmax(logits) == 3:
                if (probabilitiyFormatValue > 80):
                    print(
                        "ëŒ“ê¸€ ë‚´ìš©ì—ì„œ " + f"{probabilitiyFormatValue}ì˜ í™•ë¥ ë¡œ " + test_eval[np.argmax(logits)] + " ëŠê»´ì§‘ë‹ˆë‹¤.")
                    return np.argmax(logits)
            elif np.argmax(logits) == 4:
                if (probabilitiyFormatValue > 80):
                    print(
                        "ëŒ“ê¸€ ë‚´ìš©ì—ì„œ " + f"{probabilitiyFormatValue}ì˜ í™•ë¥ ë¡œ " + test_eval[np.argmax(logits)] + " ëŠê»´ì§‘ë‹ˆë‹¤.")
                    return np.argmax(logits)
            elif np.argmax(logits) == 5:
                if (probabilitiyFormatValue > 80):
                    print(
                        "ëŒ“ê¸€ ë‚´ìš©ì—ì„œ " + f"{probabilitiyFormatValue}ì˜ í™•ë¥ ë¡œ " + test_eval[np.argmax(logits)] + " ëŠê»´ì§‘ë‹ˆë‹¤.")
                    return np.argmax(logits)
            elif np.argmax(logits) == 6:
                if (probabilitiyFormatValue > 80):
                    print(
                        "ëŒ“ê¸€ ë‚´ìš©ì—ì„œ " + f"{probabilitiyFormatValue}ì˜ í™•ë¥ ë¡œ " + test_eval[np.argmax(logits)] + " ëŠê»´ì§‘ë‹ˆë‹¤.")
                    return np.argmax(logits)






# -----------------------------------------------------------------------------------------------


# ì´ëª¨í‹°ì½˜ ì œê±° í•¨ìˆ˜(ASCII ì½”ë“œì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
def remove_emoji(inputString):
    return inputString.encode('ascii', 'ignore').decode('ascii')


# ì´ëª¨í‹°ì½˜ ìœ ë‹ˆì½”ë“œ íŒ¨í„´
emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002500-\U00002BEF"  # chinese char
                           u"\U00002702-\U000027B0"
                           u"\U00002702-\U000027B0"
                           # u"\U000024C2-\U0001F251"
                           u"\U0001f926-\U0001f937"
                           u"\U00010000-\U0010ffff"
                           u"\u2640-\u2642"
                           u"\u2600-\u2B55"
                           u"\u200d"
                           u"\u23cf"
                           u"\u23e9"
                           u"\u231a"
                           u"\ufe0f"  # dingbats
                           u"\u3030"
                           "]+", flags=re.UNICODE)


# ì´ëª¨í‹°ì½˜ ë¬¸ìë¡œ ëŒ€ì²´
def emoticonToWord(comment):
    emoticon_list = ["â¤ï¸", "â¤", "ğŸ§¡", "ğŸ’›", "ğŸ’š", "ğŸ’™", "ğŸ’", "ğŸ’“", "ğŸ’œ", "â£ï¸", "ğŸ’•", "ğŸ’˜", "ğŸ’—", "ğŸ’“", "ğŸ’", "ğŸ’Ÿ",
                     "ğŸ˜»", "ğŸ’”", "ğŸ‘", "ğŸ‘", "ğŸ™Œ", "ğŸ˜˜", "ğŸ˜", "ğŸ˜ƒ", "ğŸ˜„", "ğŸ˜", "ğŸ˜†", "â˜ºï¸", "ğŸ˜Š", "ğŸ˜š", "ğŸ¤—", "ğŸ˜­",
                     "ğŸ˜¢", "ğŸ˜¤", "ğŸ˜ ", "ğŸ˜¡", "ğŸ¤¬", "ğŸ˜³", "ğŸ¤”", "ğŸ¤£", "ğŸ¥°", "ğŸ¤¢", "ğŸ¥³"]
    emotion_list = [" ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”",
                    " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‚¬ë‘í•´ìš”", " ì‹«ì–´í•´ìš”", " ìµœê³ ì—ìš”", "ìµœì•…ì´ì—ìš”", "ë§Œì„¸", " ì‚¬ë‘í•´ìš”",
                    " ì‚¬ë‘í•´ìš”", " ì¢‹ì•„ìš”", " ì¢‹ì•„ìš”", " ì¢‹ì•„ìš”", " ì¢‹ì•„ìš”", " ì¢‹ì•„ìš”", " ì¢‹ì•„ìš”", " ì¢‹ì•„ìš”", " ì¢‹ì•„ìš”", " ìŠ¬í¼ìš”", " ìŠ¬í¼ìš”", " ì‚ì¡Œì–´ìš”",
                    " í™”ë‚¬ì–´ìš”", " í™”ë‚¬ì–´ìš”", " í™”ë‚¬ì–´ìš”", " ì˜ ëª¨ë¥´ê² ì–´ìš”", " ê³ ë¯¼í•´ ë³¼ê²Œìš”", " ì¢‹ì•„ìš”", " ì¢‹ì•„ìš”", " ì—­ê²¨ì›Œìš”", " ì¶•í•˜í•´ìš”"]
    emodic = dict(zip(emoticon_list, emotion_list))
    templist = list(comment)  # ìˆ˜ì •ì„ ìœ„í•´ì„œ ì„ì‹œë¡œ ë¬¸ì¥-> ë¦¬ìŠ¤íŠ¸

    for index, word in enumerate(comment):
        if word in emoticon_list:  # ì •ì˜í•œ ì´ëª¨í‹°ì½˜ë¦¬ìŠ¤íŠ¸ì— ìˆë‹¤ë©´
            temp = word.replace(word, emodic[word])  # í•´ë‹¹ ì´ëª¨í‹°ì½˜ì„ ì„¤ì •í•œ ë‹¨ì–´ë¡œ ë³€í™˜
            templist[index] = temp
    comment = "".join(templist)  # ë¦¬ìŠ¤íŠ¸ -> ë¬¸ì¥
    return comment


# ìœ íŠœë¸Œ ë°ì´í„° ì „ì²˜ë¦¬
def CommentClassifyProcessing(filename):
    df = pd.read_excel(filename)

    comments = df.values  # ëŒ“ê¸€ ë°ì´í„°(ëŒ“ê¸€, ì‘ì„±ì, ë‚ ì§œ, ì¢‹ì•„ìš” ìˆ˜)
    comment_result = []  # ì „ì²˜ë¦¬ í›„ ë°ì´í„°
    PositiveNegative_dic_return = []  # ìŠ¤í”„ë§ìœ¼ë¡œ ì „ì†¡í•  json ë°ì´í„°
    Emotion_dic_return = []  # ìŠ¤í”„ë§ìœ¼ë¡œ ì „ì†¡í•  json ë°ì´í„°

    for i in comments:
        val = i[0]
        timeInComment = []
        # print(val)
        val = re.sub("<br>", " ", val)  # <br> í•œì¤„ë„ê¸° -> ìŠ¤í˜ì´ìŠ¤ ê³µë°±ìœ¼ë¡œ ë³€í™˜ , ì œê±° ì´ëª¨í‹°ì½˜ ì¶”ê°€
        val = emoticonToWord(val)  # ì´ëª¨í‹°ì½˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        val = re.sub(emoji_pattern, "", val)  # ì´ëª¨í‹°ì½˜ ì œê±°
        remove_emoji(val)  # ì´ëª¨í‹°ì½˜ ì œê±°
        soup = BeautifulSoup(val, 'html.parser')
        for j in soup.find_all('a'):  # íƒ€ì„ë¼ì¸ ì²˜ë¦¬
            timeInComment.append(j.text)
        # print(timeInComment)
        comment_result.append(val)

    for i in range(len(comment_result)):  # ë°°ì—´ í¬ê¸°ë§Œí¼ ì‹¤í–‰
        logger.debug("------------------ì „ì²˜ë¦¬ í›„-----------------")
        logger.debug("ëŒ“ê¸€: " + comments[i][0])
        logger.debug("ì‘ì„±ì: " + comments[i][1])
        logger.debug("ì‘ì„± ë‚ ì§œ: " + comments[i][2])
        logger.debug("ì¢‹ì•„ìš” ê°œìˆ˜: " + str(comments[i][3]))
        score = sentiment_predict(comment_result[i])
        if (score > 0.5):
            if (score > 0.8):
                logger.debug("{:.2f}% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.".format(score * 100))
                dic_temp = {"index": "1", "id": comments[i][1], "comment": comments[i][0],
                            "date": comments[i][2], "num_like": str(comments[i][3])}
                PositiveNegative_dic_return.append(dic_temp)
        else:
            if (score < 0.2):
                logger.debug("{:.2f}% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.".format((1 - score) * 100))
                dic_temp = {"index": "0", "id": comments[i][1], "comment": comments[i][0],
                            "date": comments[i][2], "num_like": str(comments[i][3])}
                PositiveNegative_dic_return.append(dic_temp)
        index = emotion_predict(comment_result[i])
        if index == 0:
            logger.debug("ë¶„ì„ê²°ê³¼ : ê³µí¬\n")
            dic_temp = {"index": "2", "id": comments[i][1], "comment": comments[i][0],
                        "date": comments[i][2], "num_like": str(comments[i][3])}
            Emotion_dic_return.append(dic_temp)
        elif index == 1:
            logger.debug("ë¶„ì„ê²°ê³¼ : ë†€ëŒ\n")
            dic_temp = {"index": "3", "id": comments[i][1], "comment": comments[i][0],
                        "date": comments[i][2], "num_like": str(comments[i][3])}
            Emotion_dic_return.append(dic_temp)
        elif index == 2:
            logger.debug("ë¶„ì„ê²°ê³¼ : ë¶„ë…¸\n")
            dic_temp = {"index": "4", "id": comments[i][1], "comment": comments[i][0],
                        "date": comments[i][2], "num_like": str(comments[i][3])}
            Emotion_dic_return.append(dic_temp)
        elif index == 3:
            logger.debug("ë¶„ì„ê²°ê³¼ : ìŠ¬í””\n")
            dic_temp = {"index": "5", "id": comments[i][1], "comment": comments[i][0],
                        "date": comments[i][2], "num_like": str(comments[i][3])}
            Emotion_dic_return.append(dic_temp)
        elif index == 4:
            logger.debug("ë¶„ì„ê²°ê³¼ : ì¤‘ë¦½\n")
            dic_temp = {"index": "6", "id": comments[i][1], "comment": comments[i][0],
                        "date": comments[i][2], "num_like": str(comments[i][3])}
            Emotion_dic_return.append(dic_temp)
        elif index == 5:
            logger.debug("ë¶„ì„ê²°ê³¼ : í–‰ë³µ\n")
            dic_temp = {"index": "7", "id": comments[i][1], "comment": comments[i][0],
                        "date": comments[i][2], "num_like": str(comments[i][3])}
            Emotion_dic_return.append(dic_temp)
        elif index == 6:
            logger.debug("ë¶„ì„ê²°ê³¼ : í˜ì˜¤\n")
            dic_temp = {"index": "8", "id": comments[i][1], "comment": comments[i][0],
                        "date": comments[i][2], "num_like": str(comments[i][3])}
            Emotion_dic_return.append(dic_temp)

    return PositiveNegative_dic_return, Emotion_dic_return

# CommentClassifyProcessing("bCA060Tb5pI.xlsx")
